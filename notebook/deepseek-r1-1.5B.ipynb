{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Inittial the notebook","metadata":{}},{"cell_type":"code","source":"import os\n\nos.environ['WANDB_API_KEY'] = 'none'","metadata":{"execution":{"iopub.status.busy":"2025-04-21T09:32:20.374423Z","iopub.execute_input":"2025-04-21T09:32:20.374648Z","iopub.status.idle":"2025-04-21T09:32:20.380912Z","shell.execute_reply.started":"2025-04-21T09:32:20.374624Z","shell.execute_reply":"2025-04-21T09:32:20.380246Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"### Install Dependencies ","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install unsloth\n!pip install --force-reinstall  --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n!pip install wandb","metadata":{"execution":{"iopub.status.busy":"2025-04-21T09:32:20.382146Z","iopub.execute_input":"2025-04-21T09:32:20.382385Z","iopub.status.idle":"2025-04-21T09:35:24.176348Z","shell.execute_reply.started":"2025-04-21T09:32:20.382367Z","shell.execute_reply":"2025-04-21T09:35:24.175490Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"### Load the model ","metadata":{}},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\n\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    # Can select any from the below:\n    # \"unsloth/Qwen2.5-0.5B\", \"unsloth/Qwen2.5-1.5B\", \"unsloth/Qwen2.5-3B\"\n    # \"unsloth/Qwen2.5-14B\",  \"unsloth/Qwen2.5-32B\",  \"unsloth/Qwen2.5-72B\",\n    # And also all Instruct versions and Math. Coding verisons!\n    model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"execution":{"iopub.status.busy":"2025-04-21T09:35:24.177355Z","iopub.execute_input":"2025-04-21T09:35:24.177620Z","iopub.status.idle":"2025-04-21T09:36:16.350053Z","shell.execute_reply.started":"2025-04-21T09:35:24.177595Z","shell.execute_reply":"2025-04-21T09:36:16.349450Z"},"trusted":true},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-04-21 09:35:40.418916: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745228140.699502      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745228140.787517      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n🦥 Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.51.1.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.81G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17975593e0dc474a88e4be6c8aabc749"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/236 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c5dcdae968f417ab5ba0161fff49650"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/6.78k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef5cac9e0bf84be18582d9e5d0467595"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a633003c3ca4473a7fe1071422f0aaa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01d848d6646940a68790a1837eb75986"}},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"### Configure the fine-tuning","metadata":{}},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"execution":{"iopub.status.busy":"2025-04-21T09:36:16.350732Z","iopub.execute_input":"2025-04-21T09:36:16.351021Z","iopub.status.idle":"2025-04-21T09:36:22.916216Z","shell.execute_reply.started":"2025-04-21T09:36:16.350995Z","shell.execute_reply":"2025-04-21T09:36:22.915411Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Unsloth 2025.3.19 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### Dataset Preparation","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nimport os\n\n# Define system prompt template with placeholders for Question, Response, and Reasoning Content\nsystem_prompt = \"\"\"以下是一个任务描述，配有一个提供进一步背景的提问。请根据问题提供一个适当的回答。\n在回答之前，请仔细思考问题，并简洁地创建一个逐步的推理链，确保答案逻辑清晰、准确。\n\n### Instruction:\n你是一位经验丰富的心理咨询师，擅长倾听并提供专业建议。请根据以下用户的问题，提供真诚、温暖且专业的回应，避免提供未经验证的信息。\n\n### Question:\n{}\n\n### Response:\n<think>\n{}\n</think>\n{}\"\"\"\n\n# Define the EOS token\nEOS_TOKEN = tokenizer.eos_token\n\ndef formatting_prompts_func(examples):\n    input = examples[\"input\"]\n    reasoning_content = examples[\"reasoning_content\"]\n    content = examples[\"content\"]\n    \n    # Initialize an empty list to store formatted text\n    texts = []\n    \n    # Loop through input, reasoning_content, and content to create the formatted text for each sample\n    for input_text, reasoning_text, content_text in zip(input, reasoning_content, content):\n        # Format the system_prompt with input, reasoning, and content\n        text = system_prompt.format(input_text, reasoning_text, content_text) + EOS_TOKEN\n        texts.append(text)\n    \n    # Return the formatted text as a dictionary\n    return {\n        \"text\": texts,\n    }\n\n# Load the dataset\ndataset = load_dataset(\"Kedreamix/psychology-10k-Deepseek-R1-zh\", split=\"train[0:3000]\")\n\n# Split the dataset using datasets' built-in train_test_split method\ntrain_test_split_result = dataset.train_test_split(test_size=0.2, seed=42)  # 80% for training, 20% for temp (validation + test)\ntrain_dataset = train_test_split_result[\"train\"]\ntemp_dataset = train_test_split_result[\"test\"]\n\n# Split the temp dataset into validation and test datasets\nval_test_split_result = temp_dataset.train_test_split(test_size=0.5, seed=42)  # Split remaining 20% equally for validation and test\nval_dataset = val_test_split_result[\"train\"]\ntest_dataset = val_test_split_result[\"test\"]\n\n# Apply formatting to the train, test, and validation datasets\ntrain_dataset = train_dataset.map(formatting_prompts_func, batched=True)\nval_dataset = val_dataset.map(formatting_prompts_func, batched=True)\ntest_dataset = test_dataset.map(formatting_prompts_func, batched=True)\n\n\n# Create a directory to save the formatted datasets\nsave_path = \"data\"\nos.makedirs(save_path, exist_ok=True)\n\n# Save the datasets as JSON files\ntrain_dataset.to_json(os.path.join(save_path, \"train.json\"))\nval_dataset.to_json(os.path.join(save_path, \"val.json\"))\ntest_dataset.to_json(os.path.join(save_path, \"test.json\"))\n\n# print the data\ntest_dataset [\"text\"][0]\ntest_dataset [\"text\"][0]\ntest_dataset [\"text\"][0]","metadata":{"execution":{"iopub.status.busy":"2025-04-21T09:36:22.918184Z","iopub.execute_input":"2025-04-21T09:36:22.918461Z","iopub.status.idle":"2025-04-21T09:36:22.927615Z","shell.execute_reply.started":"2025-04-21T09:36:22.918443Z","shell.execute_reply":"2025-04-21T09:36:22.926444Z"},"trusted":true},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_31/3622675359.py\"\u001b[0;36m, line \u001b[0;32m46\u001b[0m\n\u001b[0;31m    train_dataset, temp_dataset = dataset.train_test_split(test_size=0.2, seed=42)values()  # 80% for training, 20% for temp (validation + test)\u001b[0m\n\u001b[0m                                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (3622675359.py, line 46)","output_type":"error"}],"execution_count":5},{"cell_type":"markdown","source":"### Train the model ","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\nimport wandb\n\n\n\nwandb.login()\n\nrun = wandb.init(\n    # Set the wandb entity where your project will be logged (generally your team name).\n    entity=\"lucasblack997-none\",\n    # Set the wandb project where this run will be logged.\n    project=\"deepseek-r1-finetuing-v1\",\n    # Track hyperparameters and run metadata.\n    config={\n        \"learning_rate\": 1.5e-5,\n        \"architecture\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n        \"dataset\": \"psychology-10k-zh\",\n        \"epochs\": 3,  # Match withnum_train_epochs\n        \"max_seq_length\": max_seq_length,\n        \"batch_size\": 32,      \n        \"gradient_accumulation\": 16,\n    },\n)\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = train_dataset,\n    eval_dataset=val_dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False,\n    args = TrainingArguments(\n        output_dir = \"outputs\",\n        per_device_train_batch_size = 2, # 2 is enough\n        gradient_accumulation_steps = 16,\n        # batch size =  per_device_train_batch_size x  per_device_train_batch_size\n        #warmup_steps = 5,\n        warmup_ratio= 0.03,\n        num_train_epochs = 3, # Set this for 1 full training run.\n        #max_steps = 2000,\n        learning_rate = 1.5e-5,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        save_total_limit = 1,\n        logging_steps = 10,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        report_to = \"wandb\", # Use this for WandB etc\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_loss\",\n        eval_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n    ),\n)","metadata":{"execution":{"iopub.status.busy":"2025-04-21T09:36:22.928203Z","iopub.status.idle":"2025-04-21T09:36:22.928516Z","shell.execute_reply.started":"2025-04-21T09:36:22.928334Z","shell.execute_reply":"2025-04-21T09:36:22.928345Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Show current memory stats","metadata":{}},{"cell_type":"code","source":"gpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"execution":{"iopub.status.busy":"2025-04-21T09:36:22.929660Z","iopub.status.idle":"2025-04-21T09:36:22.930265Z","shell.execute_reply.started":"2025-04-21T09:36:22.930078Z","shell.execute_reply":"2025-04-21T09:36:22.930096Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Start Training","metadata":{}},{"cell_type":"code","source":"trainer_stats = trainer.train()\ntrainer.save_model(\"outputs/final_model\")\n\n# Record the final model after training \nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2025-04-21T09:36:22.931173Z","iopub.status.idle":"2025-04-21T09:36:22.931537Z","shell.execute_reply.started":"2025-04-21T09:36:22.931341Z","shell.execute_reply":"2025-04-21T09:36:22.931357Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Import Chinese for matplotlib","metadata":{}},{"cell_type":"code","source":"import matplotlib\nimport matplotlib.font_manager as fm\n!wget -O MicrosoftJhengHei.ttf https://github.com/a7532ariel/ms-web/raw/master/Microsoft-JhengHei.ttf\n!wget -O ArialUnicodeMS.ttf https://github.com/texttechnologylab/DHd2019BoA/raw/master/fonts/Arial%20Unicode%20MS.TTF\n\nfm.fontManager.addfont('MicrosoftJhengHei.ttf')\nmatplotlib.rc('font', family='Microsoft Jheng Hei')\n\nfm.fontManager.addfont('ArialUnicodeMS.ttf')\nmatplotlib.rc('font', family='Arial Unicode MS')","metadata":{"execution":{"iopub.status.busy":"2025-04-21T09:36:22.933229Z","iopub.status.idle":"2025-04-21T09:36:22.933996Z","shell.execute_reply.started":"2025-04-21T09:36:22.933809Z","shell.execute_reply":"2025-04-21T09:36:22.933826Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Display the linear Fit Plot","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n# 假設你有一個數據集，x 是特徵，y 是目標變量\n# 這裡生成一些示例數據\nnp.random.seed(42)\nx = np.random.rand(100, 1) * 10  # 特徵\ny = 2.5 * x + np.random.randn(100, 1) * 2  # 目標變量（帶噪聲）\n\n# 將數據集劃分為訓練集、驗證集和測試集\nx_train, x_temp, y_train, y_temp = train_test_split(x, y, test_size=0.4, random_state=42)\nx_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)\n\n# 訓練線性回歸模型\nlinear_model = LinearRegression()\nlinear_model.fit(x_train, y_train)\n\n# 在驗證集上預測\ny_val_pred = linear_model.predict(x_val)\n\n# 繪製驗證集上的線性擬合圖\nplt.figure(figsize=(8, 6))\nplt.scatter(x_val, y_val, color='blue', label='驗證集真實值')\nplt.plot(x_val, y_val_pred, color='red', linewidth=2, label='擬合線')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('驗證集上的線性擬合圖')\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2025-04-21T09:36:22.934878Z","iopub.status.idle":"2025-04-21T09:36:22.935213Z","shell.execute_reply.started":"2025-04-21T09:36:22.935051Z","shell.execute_reply":"2025-04-21T09:36:22.935066Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n# 假設你有一個數據集，x 是特徵，y 是目標變量\n# 這裡生成一些示例數據\nnp.random.seed(42)\nx = np.random.rand(100, 1) * 10  # 特徵\ny = 2.5 * x + np.random.randn(100, 1) * 10  # 目標變量（增加噪聲）\n\n# 將數據集劃分為訓練集、驗證集和測試集\nx_train, x_temp, y_train, y_temp = train_test_split(x, y, test_size=0.4, random_state=42)\nx_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)\n\n# 訓練線性回歸模型\nlinear_model = LinearRegression()\nlinear_model.fit(x_train, y_train)\n\n# 在所有數據集上進行預測\ny_train_pred = linear_model.predict(x_train)\ny_val_pred = linear_model.predict(x_val)\ny_test_pred = linear_model.predict(x_test)\n\n# 創建一個更細的 x 值範圍用於繪製擬合線\nx_range = np.linspace(min(x), max(x), 1000).reshape(-1, 1)\n\n# 在這個新的 x_range 上預測\ny_range_pred = linear_model.predict(x_range)\n\n# 設置繪圖\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))  # 橫向排列三個子圖\n\n# 1. 繪製訓練集的線性擬合圖\naxes[0].scatter(x_train, y_train, color='blue', label='訓練集真實值')\naxes[0].plot(x_range, y_range_pred, color='blue', linewidth=2, label='訓練集擬合線')\naxes[0].set_xlabel('X')\naxes[0].set_ylabel('Y')\naxes[0].set_title('訓練集上的線性擬合圖')\naxes[0].legend()\n\n# 2. 繪製驗證集的線性擬合圖\naxes[1].scatter(x_val, y_val, color='orange', label='驗證集真實值')\naxes[1].plot(x_range, y_range_pred, color='orange', linewidth=2, linestyle='--', label='驗證集擬合線')\naxes[1].set_xlabel('X')\naxes[1].set_ylabel('Y')\naxes[1].set_title('驗證集上的線性擬合圖')\naxes[1].legend()\n\n# 3. 繪製測試集的線性擬合圖\naxes[2].scatter(x_test, y_test, color='green', label='測試集真實值')\naxes[2].plot(x_range, y_range_pred, color='green', linewidth=2, linestyle=':', label='測試集擬合線')\naxes[2].set_xlabel('X')\naxes[2].set_ylabel('Y')\naxes[2].set_title('測試集上的線性擬合圖')\naxes[2].legend()\n\n# 顯示圖表\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2025-04-21T09:36:22.936464Z","iopub.status.idle":"2025-04-21T09:36:22.936802Z","shell.execute_reply.started":"2025-04-21T09:36:22.936671Z","shell.execute_reply":"2025-04-21T09:36:22.936685Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Inference ","metadata":{}},{"cell_type":"code","source":"# system_prompt = Copied from above\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    system_prompt.format(\n        \"你是一位经验丰富的心理咨询师，擅长倾听并提供专业建议。请根据以下用户的问题，提供真诚、温暖且专业的回应，避免提供未经验证的信息。\", # instruction\n        \"我想离开这个悲伤的世界\", # input\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)","metadata":{"execution":{"iopub.status.busy":"2025-04-21T09:36:22.937782Z","iopub.status.idle":"2025-04-21T09:36:22.938087Z","shell.execute_reply.started":"2025-04-21T09:36:22.937913Z","shell.execute_reply":"2025-04-21T09:36:22.937930Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Saving, loading finetuned models","metadata":{}},{"cell_type":"code","source":"model.save_pretrained(\"lora_model\")  # Local saving\ntokenizer.save_pretrained(\"lora_model\")\n# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving","metadata":{"execution":{"iopub.status.busy":"2025-04-21T09:36:22.938833Z","iopub.status.idle":"2025-04-21T09:36:22.939052Z","shell.execute_reply.started":"2025-04-21T09:36:22.938949Z","shell.execute_reply":"2025-04-21T09:36:22.938958Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if False:\n    from unsloth import FastLanguageModel\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n        max_seq_length = max_seq_length,\n        dtype = dtype,\n        load_in_4bit = load_in_4bit,\n    )\n    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\n# system_prompt = You MUST copy from above!\n\ninputs = tokenizer(\n[\n    system_prompt.format(\n        \"你是一位经验丰富的心理咨询师，擅长倾听并提供专业建议。请根据以下用户的问题，提供真诚、温暖且专业的回应，避免提供未经验证的信息。\", # instruction\n        \"我最近一直感到非常焦虑，不知道该如何应对\", # input\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 1024)","metadata":{"execution":{"iopub.status.busy":"2025-04-21T09:36:22.940549Z","iopub.status.idle":"2025-04-21T09:36:22.940842Z","shell.execute_reply.started":"2025-04-21T09:36:22.940692Z","shell.execute_reply":"2025-04-21T09:36:22.940704Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### GGUF / llama.cpp Conversion\nTo save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n\nSome supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n\n[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)","metadata":{}},{"cell_type":"markdown","source":"### Quantizatative and save model","metadata":{}},{"cell_type":"code","source":"# Save to q4_k_m GGUF\nif False:  model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\nif False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n\n# Save to multiple GGUF options - much faster if you want multiple!\nif False:\n    model.push_to_hub_gguf(\n        \"hf/model\", # Change hf to your username!\n        tokenizer,\n        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n    )","metadata":{"execution":{"iopub.status.busy":"2025-04-21T09:36:22.941643Z","iopub.status.idle":"2025-04-21T09:36:22.941949Z","shell.execute_reply.started":"2025-04-21T09:36:22.941795Z","shell.execute_reply":"2025-04-21T09:36:22.941809Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!jupyter nbconvert --to script deepseek-r1-1.5B.ipynb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T09:36:22.943190Z","iopub.status.idle":"2025-04-21T09:36:22.944334Z","shell.execute_reply.started":"2025-04-21T09:36:22.944216Z","shell.execute_reply":"2025-04-21T09:36:22.944227Z"}},"outputs":[],"execution_count":null}]}